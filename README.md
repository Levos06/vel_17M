# Transformer Model

–û–±—É—á–µ–Ω–Ω–∞—è transformer –º–æ–¥–µ–ª—å —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π DeepSeek (RMSNorm, RoPE, SwiGLU) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install -r requirements.txt
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏

```bash
# –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
python load_model.py --prompt "The meaning of life is"

# –° –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
python load_model.py \
    --prompt "Explain quantum physics" \
    --max-tokens 200 \
    --temperature 0.7 \
    --top-p 0.95
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–¥–µ

```python
from load_model import load_model, generate

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model, tokenizer, device = load_model()

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
text = generate(
    model, tokenizer,
    prompt="The future of AI is",
    max_new_tokens=100,
    temperature=0.8,
    top_p=0.9,
    device=device
)
print(text)
```

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
transformer/
‚îú‚îÄ‚îÄ models/                 # –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ final/              # –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model.pt        # ‚Üê –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç—É –º–æ–¥–µ–ª—å
‚îÇ   ‚îî‚îÄ‚îÄ academic_filter_model/  # –ú–æ–¥–µ–ª—å –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (SetFit)
‚îÇ
‚îú‚îÄ‚îÄ training/               # –í—Å–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –æ–±—É—á–µ–Ω–∏—è (—Å–º. training/README.md)
‚îÇ   ‚îú‚îÄ‚îÄ data/               # –î–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/        # –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ logs/               # –õ–æ–≥–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏
‚îÇ   ‚îú‚îÄ‚îÄ scripts/            # –°–∫—Ä–∏–ø—Ç—ã –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îî‚îÄ‚îÄ docs/               # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
‚îÇ
‚îú‚îÄ‚îÄ load_model.py          # –°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ tg_bot.py              # Telegram –±–æ—Ç (–ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
‚îú‚îÄ‚îÄ requirements.txt       # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ Python
‚îî‚îÄ‚îÄ README.md              # –≠—Ç–æ—Ç —Ñ–∞–π–ª
```

## üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞** —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (temperature, top-p)
- **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ Apple Silicon** (MPS backend)
- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** —Å RMSNorm, RoPE, SwiGLU
- **–ì–æ—Ç–æ–≤—ã–µ —Å–∫—Ä–∏–ø—Ç—ã** –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

## üìä –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–∏

- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: Transformer —Å RMSNorm, RoPE, SwiGLU
- **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**: ~60M (–æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å) / ~15M (SFT –≤–µ—Ä—Å–∏—è)
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç**: 512-1024 —Ç–æ–∫–µ–Ω–æ–≤
- **–°–ª–æ–≤–∞—Ä—å**: GPT-2 tokenizer (50,257 —Ç–æ–∫–µ–Ω–æ–≤)
- **–û–±—É—á–µ–Ω–∏–µ**: Pre-training + Supervised Fine-Tuning

## üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

- `temperature` (0.1-2.0): –ö–æ–Ω—Ç—Ä–æ–ª—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏. –ù–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è = –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
- `top_p` (0.0-1.0): Nucleus sampling. –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω—ã —Å –Ω–∏–∑–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
- `max_new_tokens`: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤

## ü§ó HuggingFace

–ú–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞ HuggingFace Hub:
- **Model**: [Levos06/vel_17M](https://huggingface.co/Levos06/vel_17M)

–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —Å HuggingFace:

```python
from transformers import AutoModel, AutoTokenizer
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–ø–æ—Å–ª–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–∞ HF)
model = AutoModel.from_pretrained("Levos06/vel_17M")
tokenizer = AutoTokenizer.from_pretrained("Levos06/vel_17M")
```

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏**: –°–º. `training/README.md` –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: –°–º. `training/docs/` –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π
- **–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**: –°–º. `load_model.py` –∏ `tg_bot.py`

## ‚öôÔ∏è –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Python 3.8+
- PyTorch 2.1.0+ (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π MPS –¥–ª—è Mac)
- transformers >= 4.30.0
- numpy, tqdm

## üêõ –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫

### –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞

–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª `models/final/model.pt` —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ï—Å–ª–∏ –Ω–µ—Ç, —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç –∏–∑ `training/checkpoints/`:

```bash
cp training/checkpoints/sft_step_1200.pt models/final/model.pt
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–∞–º—è—Ç—å—é

–ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑-–∑–∞ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –ø–∞–º—è—Ç–∏:
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CPU –≤–º–µ—Å—Ç–æ MPS: `device="cpu"`
- –£–º–µ–Ω—å—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ `load_model.py`

### –ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

- –ü–µ—Ä–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ–≥–¥–∞ –º–µ–¥–ª–µ–Ω–Ω–µ–µ (–∫–æ–º–ø–∏–ª—è—Ü–∏—è Metal –Ω–∞ Mac)
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à–∏–π `max_new_tokens`
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –Ω–µ –ø–µ—Ä–µ–≥—Ä–µ–≤–∞–µ—Ç—Å—è

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Å–æ–∑–¥–∞–Ω –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö.

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–∞ DeepSeek –∏ LLaMA
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö transformer –º–æ–¥–µ–ª–µ–π (RMSNorm, RoPE, SwiGLU)
