# –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏

## üéØ –û–±—â–∞—è —Å—Ö–µ–º–∞

```
–ó–∞–ø—É—Å–∫ ‚Üí –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Üí Warmup ‚Üí –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ‚Üí –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
                                     ‚Üì
                         [–ì–µ–Ω–µ—Ä–∞—Ü–∏—è + –õ–æ–≥–∏ + –ß–µ–∫–ø–æ–∏–Ω—Ç—ã]
```

---

## üìã –≠—Ç–∞–ø 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (0-2 –º–∏–Ω—É—Ç—ã)

### 1.1 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```python
dataset = TokenDataset("train.bin", context_length=1024)
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç**:
- –û—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è —Ñ–∞–π–ª `train.bin` –≤ —Ä–µ–∂–∏–º–µ memory-mapped 
- –§–∞–π–ª –ù–ï –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤ RAM (—Ç–æ–ª—å–∫–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É)
- –°–æ–∑–¥–∞–µ—Ç—Å—è –º–∞—Å—Å–∏–≤ –∏–∑ ~500M-1B —Ç–æ–∫–µ–Ω–æ–≤ —Ç–∏–ø–∞ `uint16`

**–ü–∞–º—è—Ç—å**: ~0 MB –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ (memmap –Ω–µ –∑–∞–Ω–∏–º–∞–µ—Ç RAM)

### 1.2 –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
```python
model = DeepSeekTransformer(
    vocab_size=50257,
    dim=512,
    n_layers=8,
    n_heads=8,
    max_seq_len=1024
)
```

**–ß—Ç–æ —Å–æ–∑–¥–∞–µ—Ç—Å—è**:

1. **Embeddings** (50257 √ó 512):
   - –ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ ‚Üí –≤–µ–∫—Ç–æ—Ä 512 —á–∏—Å–µ–ª
   - 25.7M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

2. **8 Transformer –±–ª–æ–∫–æ–≤**, –∫–∞–∂–¥—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç:
   
   **Attention** (Multi-Head with RoPE):
   - Q, K, V –ø—Ä–æ–µ–∫—Ü–∏–∏: 3 √ó (512 √ó 512) = 786K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   - Out –ø—Ä–æ–µ–∫—Ü–∏—è: 512 √ó 512 = 262K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   - RoPE embeddings: –Ω–µ —Ç—Ä–µ–±—É—é—Ç –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω—ã sin/cos)
   
   **SwiGLU FFN**:
   - W1: 512 √ó 2048 = 1M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   - W2: 2048 √ó 512 = 1M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤  
   - W3: 512 √ó 2048 = 1M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   
   **RMSNorm** (2 —à—Ç—É–∫–∏):
   - –¢–æ–ª—å–∫–æ –≤–µ—Å–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: 2 √ó 512 = 1K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

3. **Output Head**:
   - LM head: 512 √ó 50257 (—Å–≤—è–∑–∞–Ω —Å embeddings, –Ω–µ –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è)
   - Final RMSNorm: 512 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–ò—Ç–æ–≥–æ**: ~59.3M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
**–ü–∞–º—è—Ç—å**: ~237 MB (float32)

### 1.3 –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=3e-4,
    weight_decay=0.1,
    betas=(0.9, 0.95)
)
```

**AdamW state**:
- **Momentum** (first moment): –∫–æ–ø–∏—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (~237 MB)
- **Variance** (second moment): –µ—â–µ –æ–¥–Ω–∞ –∫–æ–ø–∏—è (~237 MB)
- **–ò—Ç–æ–≥–æ**: 474 MB –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ

**–û–±—â–∞—è –ø–∞–º—è—Ç—å –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ**: ~711 MB

---

## ‚ö° –≠—Ç–∞–ø 2: Metal Compiler Warmup (1-2 –º–∏–Ω—É—Ç—ã)

```python
for _ in range(10):
    x, y = get_batch(dataset, batch_size=32, device="mps")
    _, loss = model(x, y)
    loss.backward()
    optimizer.zero_grad()
```

**–ó–∞—á–µ–º –Ω—É–∂–Ω–æ**:
- MPS (Metal Performance Shaders) –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥
- –ü–µ—Ä–≤—ã–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ (–∏–¥–µ—Ç –∫–æ–º–ø–∏–ª—è—Ü–∏—è)
- –ü–æ—Å–ª–µ 10 –∏—Ç–µ—Ä–∞—Ü–∏–π –≤—Å–µ –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–æ ‚Üí —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç**:
1. –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ 32 —á–∞–Ω–∫–æ–≤ –ø–æ 1024 —Ç–æ–∫–µ–Ω–∞ –∏–∑ `train.bin`
2. Forward pass —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
3. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
4. Backward pass (–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)
5. –û—á–∏—Å—Ç–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–±–µ–∑ —à–∞–≥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞)

**–í—Ä–µ–º—è**: –ø–µ—Ä–≤–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è ~20-30 —Å–µ–∫, –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ ~3-5 —Å–µ–∫

---

## üîÑ –≠—Ç–∞–ø 3: –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è (50,000 —à–∞–≥–æ–≤ ‚âà 8-9 –¥–Ω–µ–π)

### 3.1 –û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è (step)

```python
for step in range(50000):
    # 1. Learning rate schedule
    lr = get_lr(step)  # Warmup ‚Üí Cosine decay
    
    # 2. Gradient accumulation (2 micro-steps)
    optimizer.zero_grad()
    for micro_step in range(2):
        x, y = get_batch(dataset, batch_size=32, device="mps")
        _, loss = model(x, y)
        loss = loss / 2  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ accumulation steps
        loss.backward()
    
    # 3. Optimizer step
    optimizer.step()
```

#### –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞:

**–®–∞–≥ 1: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate**
```python
if step < 500:  # Warmup (–ø–µ—Ä–≤—ã–µ 500 —à–∞–≥–æ–≤)
    lr = 3e-4 * (step + 1) / 500
elif step >= 50000:
    lr = 3e-5  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR
else:  # Cosine decay
    decay_ratio = (step - 500) / (50000 - 500)
    coeff = 0.5 * (1.0 + cos(œÄ * decay_ratio))
    lr = 3e-5 + coeff * (3e-4 - 3e-5)
```

–ì—Ä–∞—Ñ–∏–∫ LR:
```
3e-4 |     ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
     |    ‚ï±          ‚ï≤
     |   ‚ï±            ‚ï≤___
3e-5 |  ‚ï±                  ‚îÄ‚îÄ‚îÄ
     +‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     0   500      25000   50000
    Warmup  Cosine Decay
```

**–®–∞–≥ 2: Gradient Accumulation (2 micro-batches)**

*Micro-batch 1*:
1. **–í—ã–±–æ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö** (32 √ó 1024 —Ç–æ–∫–µ–Ω–∞):
   ```python
   # –°–ª—É—á–∞–π–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, len(dataset)]
   indices = [234512, 891234, 456789, ...]  # 32 —á–∏—Å–ª–∞
   
   # –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤ –∏–∑ train.bin
   x = train.bin[indices[0]:indices[0]+1024],
       train.bin[indices[1]:indices[1]+1024],
       ...  # 32 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
   
   y = x —Å–¥–≤–∏–Ω—É—Ç—ã–π –Ω–∞ 1 —Ç–æ–∫–µ–Ω –≤–ø—Ä–∞–≤–æ (–¥–ª—è next-token prediction)
   ```
   
   **–ü–∞–º—è—Ç—å**: 32 √ó 1024 √ó 8 bytes = 262 KB
   
2. **Forward pass**:
   ```
   Tokens [32, 1024] (int64)
      ‚Üì
   Embeddings [32, 1024, 512] (float32) ~64 MB
      ‚Üì
   Block 1 ‚Üí RMSNorm ‚Üí Attention ‚Üí Add ‚Üí RMSNorm ‚Üí SwiGLU ‚Üí Add
      ‚Üì
   Block 2 ‚Üí ...
      ‚Üì
   ...
      ‚Üì
   Block 8
      ‚Üì
   Final RMSNorm [32, 1024, 512]
      ‚Üì
   LM Head ‚Üí Logits [32, 1024, 50257] ~6.5 GB (!!)
      ‚Üì
   CrossEntropy ‚Üí Loss (1 —á–∏—Å–ª–æ)
   ```
   
   **–ê–∫—Ç–∏–≤–∞—Ü–∏–∏**: ~200-300 MB (–ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã)
   **Logits**: –û–≥—Ä–æ–º–Ω—ã–µ, –Ω–æ —Å—Ä–∞–∑—É –∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏ —É–¥–∞–ª—è—é—Ç—Å—è

3. **Backward pass**:
   - –í—ã—á–∏—Å–ª—è—é—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –í–°–ï–• 59.3M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã = –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ loss –ø–æ –∫–∞–∂–¥–æ–º—É –ø–∞—Ä–∞–º–µ—Ç—Ä—É
   - **–ü–∞–º—è—Ç—å**: 237 MB (–∫–∞–∫ –∏ —Å–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è (+=), –Ω–µ –∑–∞–º–µ–Ω—è—é—Ç—Å—è

*Micro-batch 2*:
- –¢–æ –∂–µ —Å–∞–º–æ–µ, –Ω–æ —Å –î–†–£–ì–ò–ú–ò —Å–ª—É—á–∞–π–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã **—Å—É–º–º–∏—Ä—É—é—Ç—Å—è** —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏

**–≠—Ñ—Ñ–µ–∫—Ç**: –ö–∞–∫ –±—É–¥—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏ batch_size=64 –∑–∞ —Ä–∞–∑, –Ω–æ —Å –º–µ–Ω—å—à–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏

**–®–∞–≥ 3: Optimizer step (AdamW)**

```python
for param in model.parameters():
    # 1. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ momentum
    m = 0.9 * m + 0.1 * grad
    
    # 2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ variance
    v = 0.95 * v + 0.05 * grad¬≤
    
    # 3. Bias correction
    m_hat = m / (1 - 0.9^step)
    v_hat = v / (1 - 0.95^step)
    
    # 4. Weight decay (L2 regularization)
    param = param * (1 - lr * 0.1)
    
    # 5. Adam update
    param = param - lr * m_hat / (sqrt(v_hat) + 1e-8)
```

**–ß—Ç–æ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è**:
- –í—Å–µ 59.3M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ–º–Ω–æ–≥–æ —Å–¥–≤–∏–≥–∞—é—Ç—Å—è
- –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –ø—Ä–æ—Ç–∏–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (–º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è loss)
- –í–µ–ª–∏—á–∏–Ω–∞ —à–∞–≥–∞: –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç—Å—è lr –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞

---

### 3.2 –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è

#### –ö–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
if (step + 1) % 100 == 0:
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    avg_loss = mean(–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 loss)
    tokens_per_sec = 65536 / step_time
    eta_hours = (50000 - step) * avg_step_time / 3600
    
    # –í—ã–≤–æ–¥ + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print(f"Step {step} | Loss {avg_loss:.4f} | ...")
    csv.write(f"{step},{avg_loss},{lr},...")
```

**–§–∞–π–ª—ã –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è**:
- `logs/training.log` - —Ç–µ–∫—Å—Ç
- `logs/training_metrics.csv` - CSV

#### –ö–∞–∂–¥—ã–µ 150 —à–∞–≥–æ–≤ (~1 —á–∞—Å): –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤

```python
if (step + 1) % 150 == 0:
    model.eval()  # –û—Ç–∫–ª—é—á–∏—Ç—å dropout (–µ—Å–ª–∏ –±—ã –±—ã–ª)
    
    for prompt in ["Once upon a time", "The future of", "In a world where"]:
        tokens = tokenizer.encode(prompt)
        
        for _ in range(60):  # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 60 —Ç–æ–∫–µ–Ω–æ–≤
            logits = model(tokens)[-1]  # –õ–æ–≥–∏—Ç—ã –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
            probs = softmax(logits / 0.9)  # Temperature sampling
            next_token = sample(probs)
            tokens.append(next_token)
        
        print(tokenizer.decode(tokens))
    
    model.train()  # –í–µ—Ä–Ω—É—Ç—å—Å—è –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
```

**–í—Ä–µ–º—è**: ~3-5 —Å–µ–∫—É–Ω–¥ –Ω–∞ 3 —Ç–µ–∫—Å—Ç–∞
**–í–ª–∏—è–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ**: –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ—Ç (~0.03%)

#### –ö–∞–∂–¥—ã–µ 300 —à–∞–≥–æ–≤ (~2 —á–∞—Å–∞): –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞

```python
if (step + 1) % 300 == 0:
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    torch.save({
        'step': step + 1,
        'model_state_dict': model.state_dict(),  # 59.3M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        'optimizer_state_dict': optimizer.state_dict(),  # momentum + variance
        'loss': loss,
        'tokens_processed': tokens_processed,
    }, f"checkpoints/step_{step+1}.pt")
    
    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö (–æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5)
    checkpoints = sorted(glob("checkpoints/step_*.pt"))
    if len(checkpoints) > 5:
        os.remove(checkpoints[0])  # –£–¥–∞–ª–∏—Ç—å —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π
```

**–†–∞–∑–º–µ—Ä —á–µ–∫–ø–æ–∏–Ω—Ç–∞**: ~720 MB
**–ú–∞–∫—Å–∏–º—É–º –Ω–∞ –¥–∏—Å–∫–µ**: 5 √ó 720 MB = 3.6 GB

---

## üìä –≠—Ç–∞–ø 4: –î–∏–Ω–∞–º–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è

### –¢–∏–ø–∏—á–Ω–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏—è loss:

```
–®–∞–≥        Loss      –ß—Ç–æ –º–æ–¥–µ–ª—å —É–∂–µ –∑–Ω–∞–µ—Ç
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
0          10.8      –ù–∏—á–µ–≥–æ (—Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ—Å–∞)
100        8.5       –ß–∞—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã (the, and, is, ...)
500        7.2       –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–æ–≤, —á–∞—Å—Ç—ã–µ –±–∏–≥—Ä–∞–º–º—ã
1000       6.3       –ü—Ä–æ—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
2000       5.8       –ë–∞–∑–æ–≤–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ (–ø–æ–¥–ª–µ–∂–∞—â–µ–µ-—Å–∫–∞–∑—É–µ–º–æ–µ)
5000       5.2       –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–≤—è–∑–Ω—ã–µ —Ñ—Ä–∞–∑—ã
10000      4.8       –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
20000      4.5       –°–≤—è–∑–Ω—ã–µ –∞–±–∑–∞—Ü—ã (—á–∞—Å—Ç–∏—á–Ω–æ)
30000      4.3       –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
40000      4.2       –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å
50000      4.1       –§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
```

### –ü–æ—á–µ–º—É loss –Ω–µ –ø–∞–¥–∞–µ—Ç –¥–æ 0?

1. **Perplexity** = exp(loss) = exp(4.1) ‚âà 60
   - –ú–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç ~60 –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
   - –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫ –∏–º–µ–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å!

2. **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏**:
   - 60M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (GPT-3 –∏–º–µ–µ—Ç 175B - –≤ 3000 —Ä–∞–∑ –±–æ–ª—å—à–µ)
   - 1024 —Ç–æ–∫–µ–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (vs 8K+ —É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)
   - –û–±—É—á–µ–Ω–∞ –Ω–∞ ~1.6B —Ç–æ–∫–µ–Ω–æ–≤ (vs —Ç—Ä–∏–ª–ª–∏–æ–Ω—ã —É –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π)

---

## üéì –ß—Ç–æ –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ

### –ü–µ—Ä–≤—ã–µ 1000 —à–∞–≥–æ–≤ (–ø–µ—Ä–≤—ã–µ 4 —á–∞—Å–∞):
**Loss**: 10.8 ‚Üí 6.3

**–û–±—É—á–∞–µ—Ç—Å—è**:
- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ (–∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ —á–∞—â–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è)
- –ë–∞–∑–æ–≤—ã–µ N-–≥—Ä–∞–º–º—ã ("the cat", "is a", "in the")
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è

**–ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**:
```
Before: "wkjfh kdjfg dfkjg dfg"  (–∫–∞—à–∞)
After:  "the , and is . a in to"  (–ø—Ä–æ—Å—Ç–æ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
```

### 1000-5000 —à–∞–≥–æ–≤ (4-13 —á–∞—Å–æ–≤):
**Loss**: 6.3 ‚Üí 5.2

**–û–±—É—á–∞–µ—Ç—Å—è**:
- –°–∏–Ω—Ç–∞–∫—Å–∏—Å (subject-verb agreement)
- –ö–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã
- –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

**–ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**:
```
Prompt: "The cat"
Output: "is sitting on the mat. It was very quiet."
```

### 5000-20000 —à–∞–≥–æ–≤ (13-54 —á–∞—Å–∞):
**Loss**: 5.2 ‚Üí 4.5

**–û–±—É—á–∞–µ—Ç—Å—è**:
- –î–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
- –¢–µ–º—ã –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
- –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞

**–ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**:
```
Prompt: "Scientists have discovered"
Output: "a new species of deep-sea fish that lives at extreme 
         depths. The fish has unique adaptations to survive in 
         high pressure environments."
```

### 20000-50000 —à–∞–≥–æ–≤ (54-216 —á–∞—Å–æ–≤):
**Loss**: 4.5 ‚Üí 4.1

**–û–±—É—á–∞–µ—Ç—Å—è**:
- –£–ª—É—á—à–µ–Ω–∏–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
- –ë–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∏–ª—å
- –õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

**–ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**:
```
Prompt: "The future of artificial intelligence"
Output: "depends on how we develop and deploy these systems. 
         Researchers are working on making AI more transparent 
         and aligned with human values. This includes developing 
         better methods for..."
```

---

## üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤

### –ü–∞–º—è—Ç—å (–≤ –ø–∏–∫–µ):
```
–ú–æ–¥–µ–ª—å:              237 MB
Optimizer state:     474 MB
–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã:           237 MB
–ê–∫—Ç–∏–≤–∞—Ü–∏–∏ (batch):   300 MB
–ü—Ä–æ—á–µ–µ:              ~50 MB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
–ò–¢–û–ì–û:              ~1.3 GB –∏–∑ 16 GB
```

### –î–∏—Å–∫:
```
train.bin:           ~1.6 GB
–ß–µ–∫–ø–æ–∏–Ω—Ç—ã (5 —à—Ç):    ~3.6 GB
–õ–æ–≥–∏:                ~3 MB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
–ò–¢–û–ì–û:              ~5.2 GB
```

### GPU –∑–∞–≥—Ä—É–∑–∫–∞:
- MPS (Metal): ~90-100% –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
- CPU: ~20-30% (–∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö)
- –≠–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ: ~15-20W (M2 –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π!)

---

## ‚è±Ô∏è –í—Ä–µ–º–µ–Ω–Ω–∞—è —à–∫–∞–ª–∞ (50K —à–∞–≥–æ–≤)

```
–î–µ–Ω—å 1:  [‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 3,300 —à–∞–≥–æ–≤   | Loss ~6.5
–î–µ–Ω—å 2:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 6,600 —à–∞–≥–æ–≤   | Loss ~5.8
–î–µ–Ω—å 3:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 9,900 —à–∞–≥–æ–≤  | Loss ~5.3
–î–µ–Ω—å 4:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë] 13,200 —à–∞–≥–æ–≤ | Loss ~5.0
–î–µ–Ω—å 5:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë] 16,500 —à–∞–≥–æ–≤ | Loss ~4.7
–î–µ–Ω—å 6:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 19,800 —à–∞–≥–æ–≤ | Loss ~4.6
–î–µ–Ω—å 7:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 23,100 —à–∞–≥–æ–≤ | Loss ~4.5
–î–µ–Ω—å 8:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë] 26,400 —à–∞–≥–æ–≤ | Loss ~4.3
–î–µ–Ω—å 9:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 50,000 —à–∞–≥–æ–≤ | Loss ~4.1 ‚úì
```

---

## üéØ –§–∏–Ω–∞–ª: –ß—Ç–æ –ø–æ–ª—É—á–∞–µ—Ç—Å—è

–ü–æ—Å–ª–µ 50,000 —à–∞–≥–æ–≤:
- **–ú–æ–¥–µ–ª—å**: –ú–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
- **–ö–∞—á–µ—Å—Ç–≤–æ**: –£—Ä–æ–≤–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –ª—É—á—à–µ —á–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
- **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è**: –ù–µ GPT-3, –Ω–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ

**–§–∞–π–ª—ã –Ω–∞ –≤—ã—Ö–æ–¥–µ**:
- `checkpoints/final_model.pt` - –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
- `logs/training_metrics.csv` - –≤—Å—è –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
- `logs/training_plots.png` - –≥—Ä–∞—Ñ–∏–∫–∏
- –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –Ω–∞ —Å–ª—É—á–∞–π –ø—Ä–æ–±–ª–µ–º

**–ì–æ—Ç–æ–≤–æ –∫**:
- Fine-tuning –Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö
- –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
- –ê–Ω–∞–ª–∏–∑—É –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é
